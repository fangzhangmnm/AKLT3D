{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from opt_einsum import contract\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "def _toN(t):\n",
    "    return t.detach().cpu().numpy()\n",
    "def printDiff(name,value,ref):\n",
    "    if ref is not None:\n",
    "        print(name+':',value,'diff(abs):',value-ref)\n",
    "    else:\n",
    "        print(name+':',value)\n",
    "    \n",
    "import ast\n",
    "def eval_np_array_literal(array_string):\n",
    "    array_string = ','.join(array_string.replace('[ ', '[').split())\n",
    "    return np.array(ast.literal_eval(array_string))\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRG BaseClass, can be used for HOTRG-like and XTRG-like contraction\n",
    "class SRG(torch.nn.Module):\n",
    "    def __init__(self,params,options):\n",
    "        super(SRG,self).__init__()\n",
    "        self.dtype={'float64':torch.float64,'float32':torch.float32}[options.get('dtype','float64')]\n",
    "        self.device=options.get('device','cpu')\n",
    "        self.max_dim=options.get('max_dim',16)\n",
    "        self.nLayers=options.get('nLayers',20)\n",
    "        self.use_checkpoint=options.get('use_checkpoint',True)\n",
    "        self.observable_checkerboard=False\n",
    "        self.use_bond_symmetry=False\n",
    "        \n",
    "        self.params=torch.nn.ParameterDict({\n",
    "            k:torch.nn.Parameter(torch.tensor(v,dtype=self.dtype,device=self.device)) for k,v in params.items()\n",
    "        })\n",
    "        self.persistent={}\n",
    "        self.persistent['logZ']=0\n",
    "        \n",
    "    def __str__(self):\n",
    "        rtval=\"\"\n",
    "        for k,v in self.params.items():\n",
    "            rtval+=k+':'+v+'\\n'\n",
    "        rtval+='dtype:'+self.dtype+'\\n'\n",
    "        rtval+='device:'+self.device+'\\n'\n",
    "        rtval+='max_dim:'+self.max_dim+'\\n'\n",
    "        rtval+='nLayers:'+self.nLayers+'\\n'\n",
    "        rtval+='nSite:'+2.0**nLayers+'\\n'\n",
    "        \n",
    "    def set_params(self,params):\n",
    "        self.params=torch.nn.ParameterDict({\n",
    "            k:torch.nn.Parameter(torch.tensor(v,dtype=self.dtype,device=self.device)) for k,v in params.items()\n",
    "        })\n",
    "        \n",
    "    def toT(self,t):\n",
    "        return torch.tensor(t,dtype=self.dtype,device=self.device)\n",
    "    \n",
    "    def generate_random_Isometry(self,dim1,dim2):\n",
    "        dim=max(dim1,dim2)\n",
    "        A=torch.randn(dim,dim,dtype=self.dtype,device=self.device)\n",
    "        U=torch.matrix_exp(A-A.t())\n",
    "        U=U[:dim1,:dim2]\n",
    "        return U\n",
    "    \n",
    "    def TRG_same_T(self,T,*w):\n",
    "        return self.TRG(T,T,*w)\n",
    "    \n",
    "    def _checkpoint(self,F,*ww):\n",
    "        requires_grad=False\n",
    "        for w in ww:\n",
    "            if w.requires_grad:\n",
    "                requires_grad=True\n",
    "        if self.use_checkpoint and requires_grad:\n",
    "            return torch.utils.checkpoint.checkpoint(F,*ww)\n",
    "        else:\n",
    "            return F(*ww)\n",
    "    \n",
    "    def forward_tensor(self,contract_method=None):\n",
    "        T=self.get_T0()\n",
    "        logTotal=0\n",
    "        contracted=torch.zeros((self.nLayers,2),dtype=self.dtype,device=self.device)\n",
    "        for i in range(self.nLayers):\n",
    "            w=self.ws[(i*self.w_per_layer):((i+1)*self.w_per_layer)]\n",
    "            T=self._checkpoint(self.TRG_same_T,T,*w)\n",
    "                \n",
    "            norm=torch.linalg.norm(T)\n",
    "            T=T/norm\n",
    "            logTotal=2*logTotal+torch.log(norm)\n",
    "            \n",
    "            if contract_method is not None:\n",
    "                Z=contract(T,contract_method)\n",
    "                contracted[i,0]=Z\n",
    "                contracted[i,1]=norm\n",
    "        return T,logTotal,contracted\n",
    "    \n",
    "    def forward_tensor_with_observable(self,T_op,contract_method=None,start_layer=0):\n",
    "        T=self.get_T0()\n",
    "        for i in range(start_layer):\n",
    "            w=self.ws[(i*self.w_per_layer):((i+1)*self.w_per_layer)]\n",
    "            T=self._checkpoint(self.TRG_same_T,T,*w)\n",
    "            \n",
    "        logTotal=0\n",
    "        contracted=torch.zeros((self.nLayers,3),dtype=self.dtype,device=self.device)\n",
    "        for i in range(start_layer,self.nLayers):\n",
    "            w=self.ws[(i*self.w_per_layer):((i+1)*self.w_per_layer)]\n",
    "            T1=self._checkpoint(self.TRG_same_T,T,*w)\n",
    "            T2=self._checkpoint(self.TRG,T,T_op,*w)\n",
    "            T3=self._checkpoint(self.TRG,T_op,T,*w)\n",
    "            if self.observable_checkerboard and i<self.spacial_dim:\n",
    "                T3=-T3\n",
    "\n",
    "            T,T_op=T1,(T2+T3)/2\n",
    "            norm=torch.linalg.norm(T)\n",
    "            T,T_op=T/norm,T_op/norm\n",
    "            logTotal=2*logTotal+torch.log(norm)\n",
    "            \n",
    "            if contract_method is not None:\n",
    "                Z=contract(T,contract_method)\n",
    "                Z_op=contract(T_op,contract_method)\n",
    "                contracted[i,0]=Z\n",
    "                contracted[i,1]=Z_op\n",
    "                contracted[i,2]=norm\n",
    "            \n",
    "        return T,T_op,logTotal,contracted\n",
    "    \n",
    "    \n",
    "    def dlogZ(self,param):\n",
    "        self.requires_grad_(False)\n",
    "        self.params[param].requires_grad_(True)\n",
    "        self.zero_grad()\n",
    "        logZ=self.forward()\n",
    "        logZ.backward()\n",
    "        result=_toN(self.params[param].grad)\n",
    "        self.params[param].requires_grad_(False)\n",
    "        return result\n",
    "    \n",
    "    def update_single_layer(self,layer):\n",
    "        self.requires_grad_(False)\n",
    "        \n",
    "        for i in range(layer*self.w_per_layer,(layer+1)*self.w_per_layer):\n",
    "            self.ws[i].requires_grad_(True)\n",
    "        self.zero_grad()\n",
    "        \n",
    "        logZ=self.forward()\n",
    "        logZ.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(layer*self.w_per_layer,(layer+1)*self.w_per_layer):\n",
    "                E=self.ws[i].grad\n",
    "                dim1,dim2=E.shape[0],E.shape[2]\n",
    "                E=E.reshape(dim1*dim1,dim2)\n",
    "                U,S,Vh=torch.linalg.svd(E,full_matrices=False)\n",
    "                UVh=U@Vh\n",
    "                #UVh=svd2UVh(E)\n",
    "                del U,S,Vh,E\n",
    "                \n",
    "                #calculate diff\n",
    "                UVh_old=self.ws[i].reshape(dim1*dim1,dim2)\n",
    "                self.persistent['ws_diff'][i]=_toN(torch.norm(UVh_old.t()@UVh@UVh.t()@UVh_old-torch.eye(dim2,device=UVh.device)))\n",
    "                del UVh_old\n",
    "                    \n",
    "                self.ws[i].data=UVh.reshape(dim1,dim1,dim2)\n",
    "                del UVh\n",
    "                torch.cuda.empty_cache()\n",
    "        return _toN(logZ)\n",
    "        \n",
    "    def optimize(self,nIter):\n",
    "        self.persistent['ws_diff']=np.zeros(len(self.ws))\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        if nIter>1:\n",
    "            pbar2=tqdm(range(nIter), leave=False)\n",
    "            pbar2.set_postfix({k:_toN(v) for k,v in self.params.items()})\n",
    "        else:\n",
    "            pbar2=range(nIter)\n",
    "        for i in pbar2:\n",
    "            pbar=tqdm([*range(self.nLayers-1,-1,-1)]+[*range(self.nLayers)], leave=False)\n",
    "            for j in pbar:\n",
    "                ws_shape=self.ws[j*self.w_per_layer].shape\n",
    "                if ws_shape[0]**2>ws_shape[2]:\n",
    "                    logZ=self.update_single_layer(j)\n",
    "                #else:\n",
    "                #    print(f'Skip layer {j} shape={ws_shape}')\n",
    "        #lock all grads\n",
    "        for param in self.params.values(): \n",
    "            param.requires_grad_(False)\n",
    "        for i in range(self.nLayers): #slightly faster\n",
    "            self.ws[i].requires_grad_(False)\n",
    "        \n",
    "        self.persistent['logZ_diff']=np.abs(self.persistent['logZ']-logZ)\n",
    "        self.persistent['logZ']=logZ\n",
    "        \n",
    "        # normalized by layer weight, number of elements in tensor\n",
    "        # NOT USED but multiply by number of elements in last tensor to better match the effects in output\n",
    "        self.persistent['ws_diff_normalized']=np.zeros(len(self.ws))\n",
    "        for i in range(self.nLayers):\n",
    "            for j in range(self.w_per_layer):\n",
    "                ij=i*self.w_per_layer+j\n",
    "                self.persistent['ws_diff_normalized'][ij]=self.persistent['ws_diff'][ij]/2.0**i#/torch.numel(self.ws[ij])*torch.numel(self.ws[-1])\n",
    "        # ignore the last layers we take trace directly\n",
    "        # use 10-norm so layers of large error has beter contribution       \n",
    "        self.persistent['ws_diff_total']=np.average(self.persistent['ws_diff_normalized'][:-self.w_per_layer*self.spacial_dim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOTRG(SRG):\n",
    "    def __init__(self,params,options):\n",
    "        super(HOTRG,self).__init__(params,options)\n",
    "        self.nLayers_HOSVD=options.get('nLayers_HOSVD',0)\n",
    "        self.persistent['magnetizationX']=0\n",
    "        self.persistent['magnetizationY']=0\n",
    "        self.persistent['magnetizationZ']=0\n",
    "        self.persistent['energy']=0\n",
    "    \n",
    "    def create_isometries(self,start_dim,spacial_dim):\n",
    "        ws=[]\n",
    "        bond_dim=[start_dim]*spacial_dim\n",
    "        for i in range(self.nLayers):\n",
    "            for j in range(1,spacial_dim):\n",
    "                old_dim=bond_dim[j]\n",
    "                new_dim=min(old_dim**2,self.max_dim)\n",
    "                U=self.generate_random_Isometry(old_dim**2,new_dim).view(old_dim,old_dim,new_dim)\n",
    "                ws.append(U.detach())\n",
    "                bond_dim[j]=new_dim\n",
    "            bond_dim=bond_dim[1:]+[bond_dim[0]]\n",
    "        self.ws=torch.nn.ParameterList([\n",
    "            torch.nn.Parameter(v) for v in ws\n",
    "        ])\n",
    "        self.w_per_layer=spacial_dim-1\n",
    "        self.spacial_dim=spacial_dim\n",
    "        self.TRG={2:self.HOTRG2D,3:self.HOTRG3D}[self.spacial_dim]\n",
    "        self.HOSVD={2:self.HOSVD2D,3:self.HOSVD3D}[self.spacial_dim]\n",
    "        \n",
    "        \n",
    "    def HOTRG2D(self,T1,T2,w):\n",
    "        return contract('ijkl,jmno,kna,lob->abim',T1,T2,w,w.conj())#contract and rotate\n",
    "    \n",
    "    def HOTRG3D(self,T1,T2,w1,w2):\n",
    "        return contract('ijklmn,jopqrs,kpa,lqb,mrc,nsd->abcdio',T1,T2,w1,w1.conj(),w2,w2.conj())#contract and rotate\n",
    "    \n",
    "    def HOSVD2D(self,T1,T2,BS=None):\n",
    "        MM1=contract('ijkl,jmno,ipql,pmro->knqr',T1,T2,T1.conj(),T2.conj()).reshape(T1.shape[2]*T2.shape[2],-1)\n",
    "        S1,U1=torch.linalg.eigh(MM1) #S1 ascending U S Uh=MM\n",
    "        eps1=torch.sum(torch.abs(S1[-self.max_dim:])) # might be slightly minus due to numerical error\n",
    "        \n",
    "        MM2=contract('ijkl,jmno,ipql,pmro->knqr',T1.transpose(2,3),T2.transpose(2,3),T1.conj().transpose(2,3),T2.conj().transpose(2,3)).reshape(T1.shape[3]*T2.shape[3],-1)\n",
    "        S2,U2=torch.linalg.eigh(MM2)\n",
    "        eps2=torch.sum(torch.abs(S2[-self.max_dim:]))\n",
    "\n",
    "        S,U=(S1,U1) if eps1<eps2 else (S2,U2)\n",
    "        w=U[:,-self.max_dim:].reshape(T1.shape[2],T2.shape[2],-1)\n",
    "        \n",
    "        T=contract('ijkl,jmno,kna,lob->abim',T1,T2,w,w.conj())\n",
    "        \n",
    "        assert (BS is not None) == self.use_bond_symmetry\n",
    "        if BS is not None:\n",
    "            Tsum=torch.zeros_like(T)\n",
    "            #wsum=torch.zeros_like(w)\n",
    "            for i in range(len(BS)):\n",
    "                BS[i]=[\n",
    "                    self.fix_unitary(contract('kl,no,kna,lob->ab',BS[i][1],BS[i][1],w,w.conj())),\n",
    "                    BS[i][0]\n",
    "                ]\n",
    "                Tsum+=contract('ijkl,Ii,Jj,Kk,Ll->IJKL',T,BS[i][0],BS[i][0].conj(),BS[i][1],BS[i][1].conj())\n",
    "                #wsum+=contract('kna,Aa->knA',w,BS[i][0])\n",
    "            #w=wsum/len(BS) #seems wrong\n",
    "            T=Tsum/len(BS)\n",
    "        \n",
    "        return T,[w],BS\n",
    "    \n",
    "    def HOSVD3D(self,T1,T2,BS=None):\n",
    "        \n",
    "        #if BS is not None:\n",
    "        #    T1sum=torch.zeros_like(T1)\n",
    "        #    T2sum=torch.zeros_like(T2)\n",
    "        #    for i in range(len(BS)):\n",
    "        #        T1sum+=contract('ijklmn,Ii,Jj,Kk,Ll,Mm,Nn->IJKLMN',T1,BS[i][0],BS[i][0].conj(),BS[i][1],BS[i][1].conj(),BS[i][2],BS[i][2].conj())\n",
    "        #      \n",
    "        #        T2sum+=contract('ijklmn,Ii,Jj,Kk,Ll,Mm,Nn->IJKLMN',T2,BS[i][0],BS[i][0].conj(),BS[i][1],BS[i][1].conj(),BS[i][2],BS[i][2].conj())\n",
    "        #      \n",
    "        #    T1=T1sum/len(BS)  \n",
    "        #    T2=T2sum/len(BS)  \n",
    "        \n",
    "        #print(T1.shape)\n",
    "        MM1=contract('ijklmn,jopqrs,itulmn,tovqrs->kpuv',T1,T2,T1.conj(),T2.conj()).reshape(T1.shape[2]*T2.shape[2],-1)\n",
    "        S1,U1=torch.linalg.eigh(MM1) #S ascending U S Uh=MM\n",
    "        eps1=torch.sum(torch.abs(S1[-self.max_dim:])) # might be slightly minus due to numerical error\n",
    "        \n",
    "        MM2=contract('ijklmn,jopqrs,itulmn,tovqrs->kpuv',T1.transpose(2,3),T2.transpose(2,3),T1.conj().transpose(2,3),T2.conj().transpose(2,3)).reshape(T1.shape[3]*T2.shape[3],-1)\n",
    "        S2,U2=torch.linalg.eigh(MM2)\n",
    "        eps2=torch.sum(torch.abs(S2[-self.max_dim:]))\n",
    "\n",
    "        S,U=(S1,U1) if eps1<eps2 else (S2,U2)\n",
    "        w1=U[:,-self.max_dim:].reshape(T1.shape[2],T2.shape[2],-1)\n",
    "        \n",
    "        MM1=contract('ijklmn,jopqrs,itklun,topqvs->mruv',T1,T2,T1.conj(),T2.conj()).reshape(T1.shape[4]*T2.shape[4],-1)\n",
    "        S1,U1=torch.linalg.eigh(MM1) #S ascending U S Uh=MM\n",
    "        eps1=torch.sum(torch.abs(S1[-self.max_dim:])) # might be slightly minus due to numerical error\n",
    "        \n",
    "        MM2=contract('ijklmn,jopqrs,itklun,topqvs->mruv',T1.transpose(4,5),T2.transpose(4,5),T1.conj().transpose(4,5),T2.conj().transpose(4,5)).reshape(T1.shape[5]*T2.shape[5],-1)\n",
    "        S2,U2=torch.linalg.eigh(MM2)\n",
    "        eps2=torch.sum(torch.abs(S2[-self.max_dim:]))\n",
    "\n",
    "        S,U=(S1,U1) if eps1<eps2 else (S2,U2)\n",
    "        w2=U[:,-self.max_dim:].reshape(T1.shape[4],T2.shape[4],-1)\n",
    "        #print(w1.shape,w2.shape)\n",
    "        \n",
    "        #T=contract('ijklmn,jopqrs,kpa,lqb,mrc,nsd->abcdio',T1,T2,w1,w1.conj(),w2,w2.conj())\n",
    "        \n",
    "        if BS is not None:\n",
    "            Tsum=torch.zeros_like(T)\n",
    "            #w1sum,w2sum=torch.zeros_like(w1),torch.zeros_like(w2)\n",
    "            for i in range(len(BS)):\n",
    "                BS[i]=[\n",
    "                    self.fix_unitary(contract('kl,pq,kpa,lqb->ab',BS[i][1],BS[i][1],w1,w1.conj())),\n",
    "                    self.fix_unitary(contract('mn,rs,mrc,nsd->cd',BS[i][2],BS[i][2],w2,w2.conj())),\n",
    "                    BS[i][0]\n",
    "                ]\n",
    "                Tsum+=contract('ijklmn,Ii,Jj,Kk,Ll,Mm,Nn->IJKLMN',T,BS[i][0],BS[i][0].conj(),BS[i][1],BS[i][1].conj(),BS[i][2],BS[i][2].conj())\n",
    "                #w1sum+=contract('kpa,Aa->kpA',w1,BS[i][0])\n",
    "                #w2sum+=contract('mrc,Cc->mrC',w2,BS[i][1])\n",
    "            T=Tsum/len(BS)\n",
    "            #w1,w2=w1sum/len(BS),w2sum/len(BS) #seems wrong\n",
    "        T=contract('ijklmn,jopqrs,kpa,lqb,mrc,nsd->abcdio',T1,T2,w1,w1.conj(),w2,w2.conj())\n",
    "        return T,[w1,w2],BS\n",
    "    \n",
    "    def fix_unitary(self,M):\n",
    "        U,S,Vh=torch.linalg.svd(M)\n",
    "        S/=torch.abs(S)\n",
    "        return U*S@Vh\n",
    "    \n",
    "    def fix_projector(self,P):\n",
    "        S,U=torch.linalg.eigh(P) #S ascending U S Uh=P\n",
    "        S[S<.5]=0\n",
    "        S[S>.5]=1\n",
    "        return U*S@(U.T.conj())\n",
    "    \n",
    "    def generate_isometries_HOSVD(self):\n",
    "        with torch.no_grad():\n",
    "            logTotal=0\n",
    "            T=self.get_T0()\n",
    "            BS=self.get_bond_symmetries() if self.use_bond_symmetry else None\n",
    "            for i in tqdm(range(self.nLayers), leave=False):\n",
    "                T,ww,BS=self.HOSVD(T,T,BS)\n",
    "                for j in range(self.w_per_layer):\n",
    "                    self.ws[i*self.w_per_layer+j].data=ww[j]\n",
    "                norm=torch.linalg.norm(T)\n",
    "                T=T/norm\n",
    "                logTotal=2*logTotal+torch.log(norm)\n",
    "\n",
    "            contract_all=[i for i in range(len(T.shape)//2) for j in range(2)]\n",
    "            Z=contract(T,contract_all)\n",
    "            self.persistent['logZ']=_toN((torch.log(Z)+logTotal)/2.0**self.nLayers)\n",
    "    \n",
    "    def forward(self):\n",
    "        contract_method=[i for i in range(self.spacial_dim) for j in range(2)]\n",
    "        T,logTotal,_=self.forward_tensor(contract_method=None)\n",
    "        #contract_all=[i for i in range(len(T.shape)//2) for j in range(2)]\n",
    "        Z=contract(T,contract_method)\n",
    "        return (torch.log(torch.abs(Z))+logTotal)/2.0**self.nLayers\n",
    "    \n",
    "    def forward_and_HOTRG(self):\n",
    "        contract_method=[i for i in range(self.spacial_dim) for j in range(2)]\n",
    "        T,logTotal,contracted=self.forward_tensor(contract_method=contract_method)\n",
    "        for i in range(self.nLayers_HOSVD):\n",
    "            assert False # BS not cached\n",
    "            T,_=self.HOSVD(T,T)\n",
    "            norm=torch.linalg.norm(T)\n",
    "            T=T/norm\n",
    "            logTotal=2*logTotal+torch.log(norm)\n",
    "        Z=contract(T,contract_method)\n",
    "        return (torch.log(torch.abs(Z))+logTotal)/2.0**(self.nLayers+self.nLayers_HOSVD),contracted\n",
    "    \n",
    "    def forward_with_observable_and_HOTRG(self,T_op,start_layer=0):\n",
    "        contract_method=[i for i in range(self.spacial_dim) for j in range(2)]\n",
    "        T,T_op,logTotal,contracted=self.forward_tensor_with_observable(T_op,contract_method=contract_method,start_layer=start_layer)\n",
    "        \n",
    "        for i in range(self.nLayers_HOSVD):\n",
    "            assert False # BS not cached\n",
    "            T1,ws=self.HOSVD(T,T)\n",
    "            T2=self.TRG(T,T_op,*ws)\n",
    "            T3=self.TRG(T_op,T,*ws)\n",
    "            \n",
    "            T,T_op=T1,(T2+T3)/2\n",
    "            norm=torch.linalg.norm(T)\n",
    "            T,T_op=T/norm,T_op/norm\n",
    "            logTotal=2*logTotal+torch.log(norm)\n",
    "            \n",
    "        Z=contract(T,contract_method)\n",
    "        Z_op=contract(T_op,contract_method)\n",
    "        \n",
    "        return Z_op/Z,contracted\n",
    "    \n",
    "    def calc_logZ(self):\n",
    "        with torch.no_grad():\n",
    "            a,b=self.forward_and_HOTRG()\n",
    "            self.persistent['logZ_contracted_per_layer']=_toN(b)\n",
    "            logZ=_toN(a)\n",
    "            self.persistent['logZ_diff']=np.abs(self.persistent['logZ']-logZ)\n",
    "            self.persistent['logZ']=logZ\n",
    "    \n",
    "    def calc_magnetization(self):\n",
    "        axes=[0,1,2] if hasattr(self,'get_ST0') else [2]\n",
    "        for axis in axes:\n",
    "            with torch.no_grad():\n",
    "                axisLabel=['X','Y','Z'][axis]\n",
    "                ST0=self.get_ST0(axis) if hasattr(self,'get_ST0') else self.get_SZT0()\n",
    "                a,b=self.forward_with_observable_and_HOTRG(ST0)\n",
    "                self.persistent['magnetization'+axisLabel+'_contracted_per_layer']=_toN(b)\n",
    "                magnetization=_toN(torch.abs(a))\n",
    "                self.persistent['magnetization'+axisLabel+'_diff']=np.abs(self.persistent['magnetization'+axisLabel]-magnetization)\n",
    "                self.persistent['magnetization'+axisLabel]=magnetization\n",
    "            \n",
    "    def calc_energy(self):\n",
    "        with torch.no_grad():\n",
    "            a,b=self.forward_with_observable_and_HOTRG(self.get_ET1(),start_layer=1)\n",
    "            self.persistent['energy_contracted_per_layer']=_toN(b)\n",
    "            energy=_toN(a)\n",
    "            self.persistent['energy_diff']=np.abs(self.persistent['energy']-energy)\n",
    "            self.persistent['energy']=energy\n",
    "            \n",
    "    def calc_all(self):\n",
    "        self.calc_logZ()\n",
    "        if hasattr(self,'get_ST0') or hasattr(self,'get_SZT0'):\n",
    "            self.calc_magnetization()\n",
    "        if hasattr(self,'get_ET1'):\n",
    "            self.calc_energy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "def get_CG_no_normalization(j):\n",
    "    n=int(2*j)\n",
    "    if n==0:\n",
    "        return np.eye(1)\n",
    "    CG=np.zeros((n+1,)+(2,)*n)\n",
    "    for i in range(2**n):\n",
    "        indices=tuple(map(int,bin(i)[2:].zfill(n)))\n",
    "        m=np.sum(indices)\n",
    "        CG[(m,)+indices]=1\n",
    "    return CG\n",
    "def get_CG(j):\n",
    "    n=int(2*j)\n",
    "    if n==0:\n",
    "        return np.eye(1)\n",
    "    CG=np.zeros((n+1,)+(2,)*n)\n",
    "    for i in range(2**n):\n",
    "        indices=tuple(map(int,bin(i)[2:].zfill(n)))\n",
    "        m=np.sum(indices)\n",
    "        CG[(m,)+indices]=1/np.sqrt(comb(n,m))\n",
    "    return CG\n",
    "def get_Singlet():\n",
    "    return np.array([[0,1.],[-1.,0]])\n",
    "def get_Lxyz(j):\n",
    "    n=int(2*j+1)\n",
    "    Lz=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        m=i-j\n",
    "        Lz[i,i]=m\n",
    "    Lp=np.zeros((n,n))\n",
    "    for i in range(n-1):\n",
    "        m=i-j\n",
    "        Lp[i+1,i]=np.sqrt(j*(j+1)-m*(m+1))\n",
    "    Lm=Lp.T\n",
    "    Lx=(Lp+Lm)/2\n",
    "    iLy=(Lp-Lm)/2\n",
    "    return Lx,iLy,Lz\n",
    "\n",
    "class AKLT2D(HOTRG):\n",
    "    default_params={'a1':np.sqrt(6)/2,'a2':np.sqrt(6)}\n",
    "    def __init__(self,params,options):\n",
    "        super(AKLT2D,self).__init__(params,options)\n",
    "        self.create_isometries(start_dim=4,spacial_dim=2)\n",
    "        self.observable_checkerboard=True\n",
    "        self.use_bond_symmetry=True\n",
    "        \n",
    "    def get_T0(self):\n",
    "        projector=self.toT(get_CG_no_normalization(2))\n",
    "        singlet=self.toT([[0,-1],[1,0]])\n",
    "        ac0,ac1,ac2=self.toT(1),self.params['a1'],self.params['a2']\n",
    "        deform=torch.stack([ac2,ac1,ac0,ac1,ac2])\n",
    "        node=contract('aijkl,im,kn,a->amjnl',projector,singlet,singlet,deform)\n",
    "        return contract('aijkl,amnop->imjnkolp',node,node).reshape(4,4,4,4)#UDLR\n",
    "\n",
    "    def get_ST0(self,axis):\n",
    "        projector=self.toT(get_CG_no_normalization(2))\n",
    "        singlet=self.toT([[0,-1],[1,0]])\n",
    "        ac0,ac1,ac2=self.toT(1),self.params['a1'],self.params['a2']\n",
    "        deform=torch.stack([ac2,ac1,ac0,ac1,ac2])\n",
    "        node=contract('aijkl,im,kn,a->amjnl',projector,singlet,singlet,deform)\n",
    "        op=self.toT(get_Lxyz(2)[axis])\n",
    "        return contract('aijkl,bmnop,ab->imjnkolp',node,node,op).reshape(4,4,4,4)#UDLR\n",
    "    \n",
    "    def get_bond_symmetries(self):\n",
    "        Id=self.toT(np.eye(4))\n",
    "        Swap=contract('ijkl->ijlk',Id.reshape(2,2,2,2)).reshape(4,4)\n",
    "        return [[Id]*self.spacial_dim,[Swap]*self.spacial_dim]\n",
    "    \n",
    "    \n",
    "\n",
    "class AKLT3D(HOTRG):\n",
    "    default_params={'a1':np.sqrt(20/15),'a2':np.sqrt(20/6),'a3':np.sqrt(20/1)}\n",
    "    def __init__(self,params,options):\n",
    "        super(AKLT3D,self).__init__(params,options)\n",
    "        self.create_isometries(start_dim=4,spacial_dim=3)\n",
    "        self.observable_checkerboard=True\n",
    "        self.use_bond_symmetry=True\n",
    "        \n",
    "    def get_T0(self):\n",
    "        projector=self.toT(get_CG_no_normalization(3))\n",
    "        singlet=self.toT([[0,-1],[1,0]])\n",
    "        ac0,ac1,ac2,ac3=self.toT(1),self.params['a1'],self.params['a2'],self.params['a3']\n",
    "        deform=torch.stack([ac3,ac2,ac1,ac0,ac1,ac2,ac3])\n",
    "        node=contract('aijklmn,io,kp,mq,a->aojplqn',projector,singlet,singlet,singlet,deform)\n",
    "        return contract('aijklmn,aopqrst->iojpkqlrmsnt',node,node).reshape(4,4,4,4,4,4)#UDLRFB\n",
    "\n",
    "    def get_ST0(self,axis):\n",
    "        projector=self.toT(get_CG_no_normalization(3))\n",
    "        singlet=self.toT([[0,-1],[1,0]])\n",
    "        ac0,ac1,ac2,ac3=self.toT(1),self.params['a1'],self.params['a2'],self.params['a3']\n",
    "        deform=torch.stack([ac3,ac2,ac1,ac0,ac1,ac2,ac3])\n",
    "        node=contract('aijklmn,io,kp,mq,a->aojplqn',projector,singlet,singlet,singlet,deform)\n",
    "        op=self.toT(get_Lxyz(3)[axis])\n",
    "        return contract('aijklmn,bopqrst,ab->iojpkqlrmsnt',node,node,op).reshape(4,4,4,4,4,4)#UDLRFB\n",
    "    \n",
    "    def get_bond_symmetries(self):\n",
    "        Id=self.toT(np.eye(4))\n",
    "        Swap=contract('ijkl->ijlk',Id.reshape(2,2,2,2)).reshape(4,4)\n",
    "        return [[Id]*self.spacial_dim,[Swap]*self.spacial_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f914308794c64b948afcddfd5b49491d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options={'nLayers':15,'max_dim':10,'device':'cuda:0'}\n",
    "params=AKLT3D.default_params.copy()\n",
    "params['a1']+=.008\n",
    "\n",
    "aklt=AKLT3D(params,options)\n",
    "aklt.use_bond_symmetry=False\n",
    "aklt.generate_isometries_HOSVD()\n",
    "\n",
    "\n",
    "aklt.calc_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logZ': array(3.09407501),\n",
       " 'magnetizationX': array(3.97433239e-13),\n",
       " 'magnetizationY': array(2.29447344e-16),\n",
       " 'magnetizationZ': array(1.19859808e-16),\n",
       " 'energy': 0,\n",
       " 'logZ_contracted_per_layer': array([[ 9.88481293e-02,  1.50571933e+03],\n",
       "        [-3.57110611e-01,  3.44685473e-01],\n",
       "        [ 2.46549214e+00,  3.17632194e-01],\n",
       "        [ 2.64152437e+00,  2.98910819e-01],\n",
       "        [ 2.75367040e+00,  2.80430852e-01],\n",
       "        [ 2.87228393e+00,  2.81057899e-01],\n",
       "        [ 2.93547720e+00,  2.67168063e-01],\n",
       "        [ 2.89630102e+00,  2.88754572e-01],\n",
       "        [ 2.98307306e+00,  2.97062677e-01],\n",
       "        [ 3.04846254e+00,  2.75067481e-01],\n",
       "        [ 2.47188026e+00,  3.27431947e-01],\n",
       "        [ 2.15631284e+00,  3.75473048e-01],\n",
       "        [ 1.88005917e+00,  3.96033690e-01],\n",
       "        [ 1.84161170e+00,  3.96940167e-01],\n",
       "        [ 1.81297051e+00,  3.73375485e-01]]),\n",
       " 'logZ_diff': 0.0,\n",
       " 'magnetizationX_contracted_per_layer': array([[ 9.88481293e-02,  9.86076132e-32,  1.50571933e+03],\n",
       "        [-3.57110611e-01, -1.03722785e-17,  3.44685473e-01],\n",
       "        [ 2.46549214e+00, -4.79657168e-17,  3.17632194e-01],\n",
       "        [ 2.64152437e+00, -1.25144554e-16,  2.98910819e-01],\n",
       "        [ 2.75367040e+00, -5.88941163e-17,  2.80430852e-01],\n",
       "        [ 2.87228393e+00, -1.14578777e-16,  2.81057899e-01],\n",
       "        [ 2.93547720e+00, -7.95782237e-16,  2.67168063e-01],\n",
       "        [ 2.89630102e+00, -3.13542663e-15,  2.88754572e-01],\n",
       "        [ 2.98307306e+00, -8.42583002e-15,  2.97062677e-01],\n",
       "        [ 3.04846254e+00, -2.42444624e-14,  2.75067481e-01],\n",
       "        [ 2.47188026e+00, -4.86579365e-14,  3.27431947e-01],\n",
       "        [ 2.15631284e+00, -9.75273515e-14,  3.75473048e-01],\n",
       "        [ 1.88005917e+00, -1.85510488e-13,  3.96033690e-01],\n",
       "        [ 1.84161170e+00, -3.55465657e-13,  3.96940167e-01],\n",
       "        [ 1.81297051e+00, -7.20534743e-13,  3.73375485e-01]]),\n",
       " 'magnetizationX_diff': 3.974332386582606e-13,\n",
       " 'magnetizationY_contracted_per_layer': array([[ 9.88481293e-02, -5.48144189e-18,  1.50571933e+03],\n",
       "        [-3.57110611e-01, -3.57529502e-17,  3.44685473e-01],\n",
       "        [ 2.46549214e+00,  7.12913767e-20,  3.17632194e-01],\n",
       "        [ 2.64152437e+00, -4.43371057e-18,  2.98910819e-01],\n",
       "        [ 2.75367040e+00, -1.05253004e-18,  2.80430852e-01],\n",
       "        [ 2.87228393e+00, -5.47567402e-19,  2.81057899e-01],\n",
       "        [ 2.93547720e+00, -8.11155011e-18,  2.67168063e-01],\n",
       "        [ 2.89630102e+00, -5.70222292e-18,  2.88754572e-01],\n",
       "        [ 2.98307306e+00,  1.16440094e-17,  2.97062677e-01],\n",
       "        [ 3.04846254e+00,  4.31642763e-17,  2.75067481e-01],\n",
       "        [ 2.47188026e+00,  7.88577460e-17,  3.27431947e-01],\n",
       "        [ 2.15631284e+00,  9.78065987e-17,  3.75473048e-01],\n",
       "        [ 1.88005917e+00,  1.07529372e-16,  3.96033690e-01],\n",
       "        [ 1.84161170e+00,  1.89982714e-16,  3.96940167e-01],\n",
       "        [ 1.81297051e+00,  4.15981269e-16,  3.73375485e-01]]),\n",
       " 'magnetizationY_diff': 2.294473436329451e-16,\n",
       " 'magnetizationZ_contracted_per_layer': array([[ 9.88481293e-02,  1.11022302e-16,  1.50571933e+03],\n",
       "        [-3.57110611e-01, -8.85413749e-17,  3.44685473e-01],\n",
       "        [ 2.46549214e+00, -3.85478512e-17,  3.17632194e-01],\n",
       "        [ 2.64152437e+00, -5.23191948e-16,  2.98910819e-01],\n",
       "        [ 2.75367040e+00, -2.64968892e-16,  2.80430852e-01],\n",
       "        [ 2.87228393e+00, -4.91002390e-16,  2.81057899e-01],\n",
       "        [ 2.93547720e+00, -9.16685421e-16,  2.67168063e-01],\n",
       "        [ 2.89630102e+00, -1.56079008e-15,  2.88754572e-01],\n",
       "        [ 2.98307306e+00, -1.40746206e-15,  2.97062677e-01],\n",
       "        [ 3.04846254e+00, -8.86331190e-16,  2.75067481e-01],\n",
       "        [ 2.47188026e+00,  3.01774624e-17,  3.27431947e-01],\n",
       "        [ 2.15631284e+00, -1.01080594e-16,  3.75473048e-01],\n",
       "        [ 1.88005917e+00, -1.88768081e-16,  3.96033690e-01],\n",
       "        [ 1.84161170e+00, -2.10778107e-16,  3.96940167e-01],\n",
       "        [ 1.81297051e+00, -2.17302298e-16,  3.73375485e-01]]),\n",
       " 'magnetizationZ_diff': 1.1985980793021062e-16}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aklt.persistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7dd999b5b14dd58bccddf04265d9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 0\n",
      "15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d070088a4b3b42159158652890120980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options={\n",
    "    'dtype':'float64',\n",
    "    'device':'cuda:0',\n",
    "    'max_dim':10, # 10 discussed with wei\n",
    "    'nLayers':15, # 30\n",
    "    'use_checkpoint':True\n",
    "}\n",
    "params=AKLT3D.default_params.copy()\n",
    "params['a1']+=.008\n",
    "model=AKLT3D(params,options)\n",
    "model.generate_isometries_HOSVD()\n",
    "\n",
    "# model.calc_logZ()\n",
    "model.calc_magnetization()\n",
    "# print(model.magnetization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.set_default_tensor_type(torch.cuda.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(3.5552844e-06, dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.magnetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0, 0.0),\n",
       " (-2.7897324047749574, 0.04779658942265784, -0.13333969435011256),\n",
       " (-1.821997652876098, 1.0153369211760674, -1.8499414872612385),\n",
       " (1.4392149863065409, 1.192677601453903, 1.716519477844597),\n",
       " (1.0142565934901129, 1.030020219631409, 1.0447047991892908),\n",
       " (0.8214542841935257, 0.8949524798322831, 0.7351625487078489),\n",
       " (0.72428624019466, 1.030275604994425, 0.7462144443056908),\n",
       " (0.8787220902480029, 0.9880340700172763, 0.8682073632418226),\n",
       " (1.0420737876185537, 0.9505671077898153, 0.990561066400147),\n",
       " (1.0852982191023912, 0.9742598610125194, 1.0573624920998306),\n",
       " (1.0945176855210959, 0.9899900614161253, 1.083561630710065),\n",
       " (1.0951954357030456, 0.993951124144083, 1.088570734474511),\n",
       " (1.0952695158500627, 0.9955685004135967, 1.090415829443573),\n",
       " (1.0952769848202044, 0.9967958559812493, 1.0917675596204175),\n",
       " (1.0952768607987813, 0.9973498644867291, 1.0923742286931146),\n",
       " (1.0952768238304655, 0.9974733446743865, 1.0925094368105133)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from HOTRG import trace_two_tensors\n",
    "tt=[(trace_two_tensors(T).item(),trace_two_tensors(T_op).item()) for T,T_op in zip(model.Ts,model.T_ops)]\n",
    "[((b/a if abs(a)>0 else 0),a,b) for a,b in tt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HOTRG import HOTRGLayer\n",
    "T0=model.get_T0()\n",
    "T0_op=model.get_SZT0()\n",
    "layers=[HOTRGLayer(tensor_shape=model.Ts[i].shape,ww=[model.ws[2*i],model.ws[2*i+1]]) for i in range(model.nLayers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3bb2b95f644a52bc6f8582451a0117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc654fa0ca9492ca8e04cc9f36643f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 764.00 MiB (GPU 0; 11.17 GiB total capacity; 9.37 GiB already allocated; 524.25 MiB free; 10.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mHOTRG\u001b[39;00m \u001b[39mimport\u001b[39;00m forward_observable_tensor\n\u001b[0;32m----> 2\u001b[0m Ts,T_ops,logTotals\u001b[39m=\u001b[39mforward_observable_tensor(T0,T0_op,layers)\n",
      "File \u001b[0;32m~/jupyter/AKLT3D/HOTRG.py:123\u001b[0m, in \u001b[0;36mforward_observable_tensor\u001b[0;34m(T0, T0_op, layers, start_layer, checkerboard, use_checkpoint, return_layers, cached_Ts)\u001b[0m\n\u001b[1;32m    121\u001b[0m     T1\u001b[39m=\u001b[39mforward_layer(T,T,layer\u001b[39m=\u001b[39mlayer,use_checkpoint\u001b[39m=\u001b[39muse_checkpoint)\n\u001b[1;32m    122\u001b[0m \u001b[39m#with BypassGilt(False,True):\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m T2\u001b[39m=\u001b[39mforward_layer(T,T_op,layer\u001b[39m=\u001b[39;49mlayer,use_checkpoint\u001b[39m=\u001b[39;49muse_checkpoint)\n\u001b[1;32m    124\u001b[0m \u001b[39m#with BypassGilt(True,False):\u001b[39;00m\n\u001b[1;32m    125\u001b[0m T3\u001b[39m=\u001b[39mforward_layer(T_op,T,layer\u001b[39m=\u001b[39mlayer,use_checkpoint\u001b[39m=\u001b[39muse_checkpoint)\n",
      "File \u001b[0;32m~/jupyter/AKLT3D/HOTRG.py:77\u001b[0m, in \u001b[0;36mforward_layer\u001b[0;34m(Ta, Tb, layer, use_checkpoint)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_layer\u001b[39m(Ta,Tb,layer:HOTRGLayer,use_checkpoint\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m-\u001b[39m\u001b[39m>\u001b[39mtorch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     76\u001b[0m     \u001b[39m#_forward_layer={4:_forward_layer_2D,6:_forward_layer_3D}[len(Ta.shape)]\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mreturn\u001b[39;00m _checkpoint(_forward_layer,[Ta,Tb],{\u001b[39m'\u001b[39;49m\u001b[39mlayer\u001b[39;49m\u001b[39m'\u001b[39;49m:layer},use_checkpoint\u001b[39m=\u001b[39;49muse_checkpoint)\n",
      "File \u001b[0;32m~/jupyter/AKLT3D/HOTRG.py:73\u001b[0m, in \u001b[0;36m_checkpoint\u001b[0;34m(function, args, args1, use_checkpoint)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(wrapper,\u001b[39m*\u001b[39margs)\n\u001b[1;32m     72\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs,\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49margs1)\n",
      "File \u001b[0;32m~/jupyter/AKLT3D/HOTRG.py:64\u001b[0m, in \u001b[0;36m_forward_layer\u001b[0;34m(Ta, Tb, layer)\u001b[0m\n\u001b[1;32m     60\u001b[0m insertion\u001b[39m=\u001b[39mlayer\u001b[39m.\u001b[39mget_insertion()\n\u001b[1;32m     61\u001b[0m eq\u001b[39m=\u001b[39m{\u001b[39m4\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mijkl,Jmno,jJ,xi,ym,akn,blo->abxy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     62\u001b[0m     \u001b[39m6\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mijklmn,Jopqrs,jJ,xi,yo,akp,blq,cmr,dns->abcdxy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     63\u001b[0m     }[\u001b[39mlen\u001b[39m(layer\u001b[39m.\u001b[39mtensor_shape)]\n\u001b[0;32m---> 64\u001b[0m T\u001b[39m=\u001b[39mcontract(eq,Ta,Tb,insertion,\u001b[39m*\u001b[39;49misometries)\n\u001b[1;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m T\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/opt_einsum/contract.py:507\u001b[0m, in \u001b[0;36mcontract\u001b[0;34m(*operands, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m gen_expression:\n\u001b[1;32m    505\u001b[0m     \u001b[39mreturn\u001b[39;00m ContractExpression(full_str, contraction_list, constants_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39meinsum_kwargs)\n\u001b[0;32m--> 507\u001b[0m \u001b[39mreturn\u001b[39;00m _core_contract(operands, contraction_list, backend\u001b[39m=\u001b[39;49mbackend, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49meinsum_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/opt_einsum/contract.py:573\u001b[0m, in \u001b[0;36m_core_contract\u001b[0;34m(operands, contraction_list, backend, evaluate_constants, **einsum_kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m     right_pos\u001b[39m.\u001b[39mappend(input_right\u001b[39m.\u001b[39mfind(s))\n\u001b[1;32m    572\u001b[0m \u001b[39m# Contract!\u001b[39;00m\n\u001b[0;32m--> 573\u001b[0m new_view \u001b[39m=\u001b[39m _tensordot(\u001b[39m*\u001b[39;49mtmp_operands, axes\u001b[39m=\u001b[39;49m(\u001b[39mtuple\u001b[39;49m(left_pos), \u001b[39mtuple\u001b[39;49m(right_pos)), backend\u001b[39m=\u001b[39;49mbackend)\n\u001b[1;32m    575\u001b[0m \u001b[39m# Build a new view if needed\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[39mif\u001b[39;00m (tensor_result \u001b[39m!=\u001b[39m results_index) \u001b[39mor\u001b[39;00m handle_out:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/opt_einsum/sharing.py:131\u001b[0m, in \u001b[0;36mtensordot_cache_wrap.<locals>.cached_tensordot\u001b[0;34m(x, y, axes, backend)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(tensordot)\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcached_tensordot\u001b[39m(x, y, axes\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, backend\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m currently_sharing():\n\u001b[0;32m--> 131\u001b[0m         \u001b[39mreturn\u001b[39;00m tensordot(x, y, axes, backend\u001b[39m=\u001b[39;49mbackend)\n\u001b[1;32m    133\u001b[0m     \u001b[39m# hash based on the (axes_x,axes_y) form of axes\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     _save_tensors(x, y)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/opt_einsum/contract.py:374\u001b[0m, in \u001b[0;36m_tensordot\u001b[0;34m(x, y, axes, backend)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[39m\"\"\"Base tensordot.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    373\u001b[0m fn \u001b[39m=\u001b[39m backends\u001b[39m.\u001b[39mget_func(\u001b[39m'\u001b[39m\u001b[39mtensordot\u001b[39m\u001b[39m'\u001b[39m, backend)\n\u001b[0;32m--> 374\u001b[0m \u001b[39mreturn\u001b[39;00m fn(x, y, axes\u001b[39m=\u001b[39;49maxes)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/opt_einsum/backends/torch.py:54\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(x, y, axes)\u001b[0m\n\u001b[1;32m     51\u001b[0m torch, _ \u001b[39m=\u001b[39m _get_torch_and_device()\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m _TORCH_HAS_TENSORDOT:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensordot(x, y, dims\u001b[39m=\u001b[39;49maxes)\n\u001b[1;32m     56\u001b[0m xnd \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mndimension()\n\u001b[1;32m     57\u001b[0m ynd \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mndimension()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/torch/functional.py:1092\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(a, b, dims, out)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     dims_b \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(dims))\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1092\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mtensordot(a, b, dims_a, dims_b)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1094\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mtensordot(a, b, dims_a, dims_b, out\u001b[39m=\u001b[39mout)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 764.00 MiB (GPU 0; 11.17 GiB total capacity; 9.37 GiB already allocated; 524.25 MiB free; 10.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from HOTRG import forward_observable_tensor\n",
    "Ts,T_ops,logTotals=forward_observable_tensor(T0,T0_op,layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
